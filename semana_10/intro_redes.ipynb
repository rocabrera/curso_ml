{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Aula 10 - Introdução a Redes Neurais**\n",
    "\n",
    "## **TOC:**\n",
    "Na aula de hoje, vamos explorar os seguintes tópicos em Python:\n",
    "\n",
    "- 1) [Introdução](#intro)\n",
    "- 2) [Redes Neurais Artificiais](#rna)\n",
    "    - 2.1) [Como a rede neural é treinada?](#how)\n",
    "- 3) [O exemplo clássico: MNIST](#MNIST)\n",
    "- 4) [Overfitting](#overfitting)\n",
    "- 5) [Outras arquiteturas de RNAs](#arch)\n",
    "- 6) [Quando usar Redes Neurais?](#when)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) **Introdução** <a class=\"anchor\" id=\"intro\"></a>\n",
    "\n",
    "Seres humanos são muito bons em aprender! Isto se deve a nosso sistema nervoso, que pode ser entendido como uma grande rede de neurônios interligados.\n",
    "\n",
    "Um dos objetivos da Inteligência Artificial é o de construir sistemas inteligentes, com capacidade cognitiva similar à dos humanos.\n",
    "\n",
    "Assim sendo, faz sentido construirmos um modelo inspirado no sistema nervoso, não é mesmo?\n",
    "\n",
    "Assim nasceram as __Redes Neurais Artificiais (RNAs)__! A imagem a seguir ilustra a inspiração biológica para a construção deste modelo:\n",
    "\n",
    "<center><img src=\"https://www.ee.co.za/wp-content/uploads/2019/07/Application-of-machine-learning-algorithms-in-boiler-plant-root-cause-analysis-Fig-1.jpg\" width=\"400\"/></center>\n",
    "\n",
    "Vamos entender um pouco melhor como funcionam as RNAs?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) **Redes neurais artificiais** <a class=\"anchor\" id=\"rna\"></a>\n",
    "\n",
    "Uma RNA é composta pelos seguintes elementos básicos:\n",
    "\n",
    "- **Unidades (ou neurônios)**: são as unidades mínimas de processamento da rede neural, onde as operações matemáticas são realizadas;\n",
    "\n",
    "- **Camadas**: há três tipos de camadas:\n",
    "\n",
    "\t- **Camada de entrada (input layer)**: é a camada de entrada de dados. O número de unidades nesta camada é igual ao número de features do modelo;\n",
    "\n",
    "\t- **Camadas ocultas (hidden layers)**: são as camadas de processamento. O número de camadas ocultas, bem como o número de neurônios em cada uma delas, é variável, dependendo do problema e dos dados. Em geral, a melhor estratégia é experimentar diferentes números de camadas e de neurônios;\n",
    "\n",
    "\t- **Camada de saída (output layer)**: é a camada que dá a resposta da rede neural, isto é, o valor predito por ela. O número de unidades nesta camada depende do tipo de output desejado, e é o que determina o target (variável dependente) do modelo.\n",
    "\t\n",
    "\t\n",
    "As camadas e neurônios são interligadas entre si através de conexões. A cada conexão, associa-se um **peso** (que denotamos pela letra **W**). O objetivo da RNA é **\"aprender\" os pesos que melhor se ajustam aos dados!**\n",
    "\n",
    "Após treinada, a rede neural faz previsões **passando os dados de input através de todos os nós**, conforme ilustrado a seguir:\n",
    "\n",
    "\n",
    "<center><img src=\"https://thumbs.gfycat.com/BouncyGleefulFeline-max-1mb.gif\" width=500></center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) **Como a rede neural é treinada?** <a class=\"anchor\" id=\"how\"></a>\n",
    "\n",
    "A \"aprendizagem\" da Rede Neural a partir dos dados se dá através de duas etapas:\n",
    "\n",
    "- **Forward Propagation**;\n",
    "- **Backward Propagation**.\n",
    "\n",
    "No __forward propagation__, a informação propaga na direção habitual (de frente pra trás) na rede neural: features são lidas na camada de input, passam pelo processamento nas camadas ocultas, e a resposta (target) é predita na camada de output. \n",
    "\n",
    "Para que a predição seja realizada, os neurônios nas camadas ocultas realizam as seguintes duas etapas de cálculo:\n",
    "\n",
    "- Uma combinação linear entre o output (que denotamos pela letra **a**) da camada enterior e os pesos da camada atual. Isto é, se tivermos n ligações. a combinação linear é:\n",
    "\n",
    "$$z_j = \\sum_{i=1}^{D} w_{ji}x_i + x_0$$\n",
    "\n",
    "- Aplica-se uma **função de ativação** não-linear à combinação linear acima. As principais funções de ativação utilizadas são:\n",
    "\n",
    "$$a_j = f(z_j)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/1000/1*4ZEDRpFuCIpUjNgjDdT2Lg.png\n",
    "\" width=\"500\" /></center>\n",
    "\n",
    "O cálculo realizado por um único neurônio é bem parecido com um **perceptron**, ilustrado a seguir:\n",
    "\n",
    "\n",
    "<center><img src=\"https://img2.gratispng.com/20180619/oav/kisspng-multilayer-perceptron-machine-learning-statistical-5b2996bdb9dcd2.4724873615294522217613.jpg\" width=\"400\" /></center>\n",
    "\n",
    "\n",
    "A escolha das funções de ativação também pode ser variável, mas costuma-se utilizar:\n",
    "\n",
    "- **ReLu** nas camadas ocultas;\n",
    "- **Sigmoid** (para problemas de classificação binários) ou **Softmax** (para probelams de classificação multiclasse) na camada de output.\n",
    "\n",
    "\n",
    "Ao fim do forward propagation, na camada de output, calculamos a **função de perda**, que quantifica qual a **diferença entre as predições feitas pela rede neural e os valores reais do target dos dados**. <font color=\"orange\"><b>Cada tipo de problema tem uma função de perda própria.</b></font>\n",
    "\n",
    "Queremos que as predições sejam sempre o mais próximas o possível dos valores reais!\n",
    "\n",
    "Então, o que fazemos é **minimizar** a função de perda. \n",
    "\n",
    "Isto é feito ao propagarmos a informação na direção contrária (de trás pra frente) na rede neural, o que caracteriza o chamado __backward propagation__. \n",
    "\n",
    "Para minimizar a função de perda, utilizamos um **otimizador**, que são objetos que representam o procedimento matemático de minimização da função de perda.\n",
    "\n",
    "Os principais otimizadores utilizados são: __gradiente descendente (GD)__, **Adam** e **RMSProp** (vale a pena testar cada um deles!)\n",
    "\n",
    "\n",
    "Este processo de forward e backward propagation é feito iterativamente, várias vezes. Cada rodada é chamada de **epoch**.\n",
    "\n",
    "O objetivo do backward propagation é **determinar os pesos que miminizem a função de perda!** A cada epoch, os pesos são **atualizados**, de modo que a função de perda é sempre reduzida em direção ao seu mínimo!\n",
    "\n",
    "Para quem quiser saber mais, segue uma sugestão de leitura adicional: [Neural Networks Explained](https://medium.com/datadriveninvestor/neural-networks-explained-6e21c70d7818)\n",
    "_______\n",
    "\n",
    "Vamos agora ao nosso exemplo prático: construiremos nossa própria rede neural!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) **O exemplo clássico: MNIST** <a class=\"anchor\" id=\"MNIST\"></a>\n",
    "\n",
    "Neste exemplo, usaremos o **[MNIST](https://www.kaggle.com/c/digit-recognizer/data)**, o famoso dataset de dígitos (números de 0 a 9) escritos à mão -- caso você queira saber mais, [clique aqui!](http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "\n",
    "<center><img src=\"https://i2.wp.com/syncedreview.com/wp-content/uploads/2019/06/MNIST.png?fit=530%2C297&ssl=1\" width=450></center>\n",
    "\n",
    "O objetivo do nosso modelo será o de **classificar digítos, com base em imagens**. \n",
    "\n",
    "Assim sendo, temos um **problema de classificação multiclasse** (pois os dados serão classificados em uma dentre 10 classes possíveis, de 0 a 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importe as bibliotecas que sempre usamos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A base do MNIST é composta por imagens de números manuscritos.\n",
    "\n",
    "Cada imagem é uma **matriz 28 x 28**, contendo assim **784 pixels**. \n",
    "\n",
    "As imagens estão em escala de cinza, na qual cada pixel pode variar de **0 a 255**, e foram centralizadas, de forma que o número não fique \"cortado\" por estar na borda. \n",
    "\n",
    "- o primeiro da forma (a, 784), que são os 784 pixels da imagem organizados de forma sequencial;\n",
    "- o segundo da forma (a, 1), que é o identificador (label) da imagem, sendo um número que varia de 0 a 9.\n",
    "\n",
    "Para maiores detalhes, verificar a [página da base](http://yann.lecun.com/exdb/mnist/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Pré-processamento dos dados\n",
    "\n",
    "__Rescalamento__\n",
    "\n",
    "É interessante que o os valores sejam rescalados **entre 0 e 1 para que o tempo de treinamento seja otimizado**. \n",
    "\n",
    "Algumas respostas sobre o porquê disso podem ser vistas [nesse link](https://stackoverflow.com/questions/4674623/why-do-we-have-to-normalize-the-input-for-an-artificial-neural-network). \n",
    "\n",
    "Então, fazemos a divisão correpondentes aos pixels das imagens pelo pixel de valor máximo do **conjunto de treino**.\n",
    "\n",
    "Obs: isso também poderia ser feito com o MinMaxScaler!\n",
    "\n",
    "Não vamos implementar para tornar o código mais simples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_size = 784  # 28x28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "hidden_size = 100\n",
    "num_classes = 10\n",
    "num_epochs = 2\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset\n",
    "\n",
    "# train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "# test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, transform=transforms.ToTensor())\n",
    "test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que está acontecendo acima?\n",
    "\n",
    "O treinamento de uma rede neural (e de todo modelo de machine learning, na verdade) consiste em um **problema de otimização**, em que queremos encontrar **o valor mínimo da função de perda**\n",
    "\n",
    "No nosso caso, a função de perda é a \"categorical crossentropy\", cuja expressão matemática é:\n",
    "\n",
    "$$ L(y, \\hat{y}) = - \\sum_i{y_i \\log \\hat{y}_i}$$\n",
    "\n",
    "onde $\\hat{y}$, a previsão do modelo, é uma expressão bem complicada, que relaciona todos os pesos e funções de ativação de toda a rede neural.\n",
    "\n",
    "Como mencionamos, as redes neurais utilizam o procedimento de **back propagation** em seu treinamento (cujo objetivo é **determinar os pesos**):\n",
    "\n",
    "<center><img src=\"https://machinelearningknowledge.ai/wp-content/uploads/2019/10/Backpropagation.gif\" width=300></center>\n",
    "\n",
    "Matematicamente, o back propagation é implementado através de algum **método de otimização**, com o fim de **determinar os pesos que minimizam a função de perda**. O principal método utilizado para este fim é o **gradiente descendente**:\n",
    "\n",
    "<center><img src=\"https://thumbs.gfycat.com/AngryInconsequentialDiplodocus-size_restricted.gif\" width=500></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto de Validação\n",
    "\n",
    "Em modelos de rede neural, é importante que exista um terceiro conjunto além de treino e teste, o **conjunto de validação**.\n",
    "\n",
    "Durante o treinamento, a depender do caso, **risco de ocorrer overfitting é bastante alto**.\n",
    "\n",
    "Uma forma de verificar e evitar isso é por meio do conjunto de validação, o qual é um subconjunto do conjunto de treino que não é utilizado pelo otimizador.  \n",
    "\n",
    "Espera-se que a o valor da função de perda (loss) vá diminuindo a cada época, tanto para o conjunto de treino como para o conjunto de validação. Porém, se a função de perda diminui para o conjunto de treino e aumenta para o conjunto de validação (ocorrendo assim um descolamento), podemos concluir que está ocorrendo um overfitting para o conjunto de treino.\n",
    "\n",
    "Deve-se então interromper o processo de treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Arquitetura da Rede\n",
    "\n",
    "Faremos uma rede simples, com:\n",
    "\n",
    "- **3 camadas**;\n",
    "- **25 neurônios cada**; \n",
    "- **Camadas densas**;\n",
    "- **função de ativação ReLu nas camadas escondidas**;\n",
    "- **10 neurônios e ativação Softmax na camada de saída**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diferentes funções de perda possuem diferentes propósitos, devendo ser escolhido caso a caso. Já os otimizadores possuem maior liberdade de escolha, não existindo uma regra fechada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Modelo treinado!__\n",
    "\n",
    "Agora, devemos **fazer predições** e **avaliar a performance**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) **Overfitting** <a class=\"anchor\" id=\"overfitting\"></a>\n",
    "\n",
    "O **overfitting** é algo que pode ser bastante comum em redes neurais se não for bem tratado, porque ele é um modelo altamente não linear!\n",
    "\n",
    "Relembrando, o overfitting está intimamente relacionado com o tradeoff viés-variância:\n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2017/02/Bias-Variance-Tradeoff-In-Machine-Learning-1.png\" width=500>\n",
    "\n",
    "Podemos visualizar esta característica em nosso modelo ao **plotar** o valor da **função de perda** a cada epoch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "A curva acima é muito parecida com o nosso exemplo do tradeoff vié-variância, não é mesmo?!\n",
    "\n",
    "Temos, portanto, um forte indício que está começando a ocorrer overfitting!\n",
    "\n",
    "Para evitarmos isso, pode ser interessante que o **treinamento seja interrompido antes do overfitting começar a ocorrer**! Esta técnica é conhecida como **early stopping**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) **Outras arquiteturas de RNAs** <a class=\"anchor\" id=\"arch\"></a>\n",
    "\n",
    "Os modelos de redes neurais formam uma classe enorme de modelos, de grande variedade e aplicabilidade!\n",
    "\n",
    "Além da rede neural simples que construímos (uma **rede neural densa (fully-connected)**), há muitas outras arquiteturas!\n",
    "\n",
    "A imagem a seguir ilustra esta enorme diversidade:\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/4000/1*cuTSPlTq0a_327iTPJyD-Q.png\" width=500></center>\n",
    "\n",
    "__Para saber um pouco mais dessas muitas arquiteturas, [clique aqui!](https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464)__\n",
    "\n",
    "Vamos mencionar duas arquiteturas muito especiais:\n",
    "\n",
    "### Redes Neurais Convolucionais (CNN)\n",
    "\n",
    "Utilizam-se de **convoluções** nas camadas iniciais.\n",
    "\n",
    "<center><center><img src=\"https://miro.medium.com/max/2340/1*Fw-ehcNBR9byHtho-Rxbtw.gif\" width=300></center>\n",
    "\n",
    "> Estas redes são especializadas para modelos que envolvam **imagens**, sendo, portanto, a principal escolha para modelos de **visão computacional**\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/3288/1*uAeANQIOQPqWZnnuH-VEyw.jpeg\" width=600></center>\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/1200/1*XbuW8WuRrAY5pC4t-9DZAQ.jpeg\" width=600></center>\n",
    "\n",
    "### Redes Neurais Recorrentes (RNN)\n",
    "\n",
    "São redes neurais em que as conexões são feitas de maneira **sequencial e cíclica**\n",
    "\n",
    "> Estas redes são especializadas para modelos que envolvam **dados sequenciais**, como, por exemplo, **textos**, **áudios**, **filmes (sequência de quadros)**, **séries temporais**, etc.\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/1200/1*chs1MCz2rCK4_dFRLnUEIg.png\" width=300></center>\n",
    "\n",
    "Modelos de RNNs mais sofisticados se utilizam de **células de memória (ex.: LSTM, GRU)** para reter a informação de dados do início da sequência, e propagá-los em todo o modelo:\n",
    "\n",
    "<center><img src=\"https://www.researchgate.net/profile/Sergio_Guadarrama/publication/319770438/figure/fig1/AS:613925582303265@1523382673457/A-diagram-of-a-basic-RNN-cell-left-and-an-LSTM-memory-cell-right-used-in-this-paper.png\" width=400></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6) **Quando usar Redes Neurais?**  <a class=\"anchor\" id=\"when\"></a>\n",
    "\n",
    "No atual cenário de Big Data em que vivemos (há muitos dados em todo o lugar!), os modelos de Redes Neurais e Deep Learning são cada vez mais utilizados!\n",
    "\n",
    "Isto é possível porque a performance destes modelos aumenta conforme mais dados são utilizados, diferentemente dos modelos tradicionais, cuja performance é estabilizada depois de certa quantidade de dados!\n",
    "\n",
    "\n",
    "<center><img src=\"https://www.sumologic.com/wp-content/uploads/performances_vs_data.png\" width=\"400\" /></center>\n",
    "\n",
    "\n",
    "Então, é de se esperar que os modelos de Deep Learning funcionem melhor nos cenários em que há **muitos dados disponíveis**.\n",
    "\n",
    "No entanto, se houver tempo e recursos computacionais disponíveis, é sempre uma ideia construir também um modelo de Deep Learning juntamente de outros modelos de Machine Learning, e então comparar qual tem melhor performance! :)\n",
    "\n",
    "\n",
    "### **Mas e o tal do \"Deep Learning\"?**\n",
    "\n",
    "De forma geral, uma rede neural é chamada de \"profunda\" (deep), se ela tiver mais de uma camada oculta. Neste caso, temos uma **\"Rede Neural Profunda\" (Deep Neural Network)**.\n",
    "\n",
    "<center><img src=\"https://thedatascientist.com/wp-content/uploads/2018/03/simple_neural_network_vs_deep_learning.jpg\" width=500></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "O termo \"Deep Learning\" é utilizado pra se referir a modelos que usam Redes Neurais Profundas, mas este termo engloba também outras **arquiteturas** de redes neurais mais especializadas e complexas, como CNNs e RNNs, que inevitavelmente são \"profundas\", por construção."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_playground",
   "language": "python",
   "name": "venv_playground"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
